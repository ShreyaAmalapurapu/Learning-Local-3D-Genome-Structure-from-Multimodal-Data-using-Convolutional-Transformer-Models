{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"A100"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"8337407c76fb4519bb60a1970244f91b":{"model_module":"@jupyter-widgets/output","model_name":"OutputModel","model_module_version":"1.0.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/output","_model_module_version":"1.0.0","_model_name":"OutputModel","_view_count":null,"_view_module":"@jupyter-widgets/output","_view_module_version":"1.0.0","_view_name":"OutputView","layout":"IPY_MODEL_181dff1a71e94c2f8d959da581fc4163","msg_id":"","outputs":[{"output_type":"display_data","data":{"text/plain":"","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"},"metadata":{}}]}},"181dff1a71e94c2f8d959da581fc4163":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A53NVPN_Xd1x","executionInfo":{"status":"ok","timestamp":1765594478917,"user_tz":300,"elapsed":13642,"user":{"displayName":"Sneha Bapana","userId":"06343812349805612711"}},"outputId":"f8d2c8bd-ac68-403a-e0f1-7053efc7338b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","import os\n","\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import os\n","os.environ['PYTORCH_JIT'] = '0'"],"metadata":{"id":"pMQn1aDgb_b3","executionInfo":{"status":"ok","timestamp":1765594480530,"user_tz":300,"elapsed":4,"user":{"displayName":"Sneha Bapana","userId":"06343812349805612711"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["!pip install pytorch_lightning"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nvvPdTNRcE3k","executionInfo":{"status":"ok","timestamp":1765594487977,"user_tz":300,"elapsed":6098,"user":{"displayName":"Sneha Bapana","userId":"06343812349805612711"}},"outputId":"0a5e2bca-b500-434a-88f6-2159aba8a84a"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pytorch_lightning\n","  Downloading pytorch_lightning-2.6.0-py3-none-any.whl.metadata (21 kB)\n","Requirement already satisfied: torch>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from pytorch_lightning) (2.9.0+cu126)\n","Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.12/dist-packages (from pytorch_lightning) (4.67.1)\n","Requirement already satisfied: PyYAML>5.4 in /usr/local/lib/python3.12/dist-packages (from pytorch_lightning) (6.0.3)\n","Requirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2022.5.0->pytorch_lightning) (2025.3.0)\n","Collecting torchmetrics>0.7.0 (from pytorch_lightning)\n","  Downloading torchmetrics-1.8.2-py3-none-any.whl.metadata (22 kB)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from pytorch_lightning) (25.0)\n","Requirement already satisfied: typing-extensions>4.5.0 in /usr/local/lib/python3.12/dist-packages (from pytorch_lightning) (4.15.0)\n","Collecting lightning-utilities>=0.10.0 (from pytorch_lightning)\n","  Downloading lightning_utilities-0.15.2-py3-none-any.whl.metadata (5.7 kB)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2022.5.0->pytorch_lightning) (3.13.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from lightning-utilities>=0.10.0->pytorch_lightning) (75.2.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch_lightning) (3.20.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch_lightning) (1.14.0)\n","Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch_lightning) (3.6.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch_lightning) (3.1.6)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch_lightning) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch_lightning) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch_lightning) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch_lightning) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch_lightning) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch_lightning) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch_lightning) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch_lightning) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch_lightning) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch_lightning) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch_lightning) (2.27.5)\n","Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch_lightning) (3.3.20)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch_lightning) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch_lightning) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch_lightning) (1.11.1.6)\n","Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch_lightning) (3.5.0)\n","Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics>0.7.0->pytorch_lightning) (2.0.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.4.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (25.4.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.8.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (6.7.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (0.4.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.22.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.1.0->pytorch_lightning) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.1.0->pytorch_lightning) (3.0.3)\n","Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.12/dist-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (3.11)\n","Downloading pytorch_lightning-2.6.0-py3-none-any.whl (849 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.5/849.5 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading lightning_utilities-0.15.2-py3-none-any.whl (29 kB)\n","Downloading torchmetrics-1.8.2-py3-none-any.whl (983 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.2/983.2 kB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: lightning-utilities, torchmetrics, pytorch_lightning\n","Successfully installed lightning-utilities-0.15.2 pytorch_lightning-2.6.0 torchmetrics-1.8.2\n"]}]},{"cell_type":"code","source":["import gc\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import pytorch_lightning as pl\n","from torch.utils.data import DataLoader\n","import pytorch_lightning.callbacks as callbacks\n"],"metadata":{"id":"CfuuFSaUcB7n","executionInfo":{"status":"ok","timestamp":1765594505763,"user_tz":300,"elapsed":14604,"user":{"displayName":"Sneha Bapana","userId":"06343812349805612711"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["!pip install pyBigWig"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sRa4GxEdc-lc","executionInfo":{"status":"ok","timestamp":1765594511794,"user_tz":300,"elapsed":4398,"user":{"displayName":"Sneha Bapana","userId":"06343812349805612711"}},"outputId":"fbda81f7-e970-483e-e88b-0b43b673dc98"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pyBigWig\n","  Downloading pyBigWig-0.3.24-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n","Downloading pyBigWig-0.3.24-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (187 kB)\n","\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/187.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m187.1/187.1 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pyBigWig\n","Successfully installed pyBigWig-0.3.24\n"]}]},{"cell_type":"code","source":["\"\"\"\n","End-to-End Hi-C Prediction Training with C.Origami Architecture\n","SPEED OPTIMIZED VERSION FOR A100\n","WITH DNA METHYLATION DATA\n","\"\"\"\n","import os\n","import argparse\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import pytorch_lightning as pl\n","from torch.utils.data import DataLoader\n","import pytorch_lightning.callbacks as callbacks\n","import gc\n","\n","# ================================================================\n","# Model Components (C.Origami-style architecture)\n","# ================================================================\n","\n","class ConvBlock(nn.Module):\n","    def __init__(self, size, stride=2, hidden_in=64, hidden=64):\n","        super().__init__()\n","        pad_len = int(size / 2)\n","        self.scale = nn.Sequential(\n","            nn.Conv1d(hidden_in, hidden, size, stride, pad_len),\n","            nn.BatchNorm1d(hidden),\n","            nn.ReLU(),\n","        )\n","        self.res = nn.Sequential(\n","            nn.Conv1d(hidden, hidden, size, padding=pad_len),\n","            nn.BatchNorm1d(hidden),\n","            nn.ReLU(),\n","            nn.Conv1d(hidden, hidden, size, padding=pad_len),\n","            nn.BatchNorm1d(hidden),\n","        )\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, x):\n","        scaled = self.scale(x)\n","        identity = scaled\n","        res_out = self.res(scaled)\n","        return self.relu(res_out + identity)\n","\n","\n","class EncoderSplit(nn.Module):\n","    \"\"\"Separate encoders for DNA sequence and epigenomic features\"\"\"\n","    def __init__(self, num_epi, output_size=256, filter_size=5, num_blocks=12):\n","        super().__init__()\n","        self.filter_size = filter_size\n","\n","        self.conv_start_seq = nn.Sequential(\n","            nn.Conv1d(5, 16, 3, 2, 1),\n","            nn.BatchNorm1d(16),\n","            nn.ReLU(),\n","        )\n","\n","        self.conv_start_epi = nn.Sequential(\n","            nn.Conv1d(num_epi, 16, 3, 2, 1),\n","            nn.BatchNorm1d(16),\n","            nn.ReLU(),\n","        )\n","\n","        hiddens = [32, 32, 32, 32, 64, 64, 128, 128, 128, 128, 256, 256]\n","        hidden_ins = [32, 32, 32, 32, 32, 64, 64, 128, 128, 128, 128, 256]\n","        hiddens_half = (np.array(hiddens) / 2).astype(int)\n","        hidden_ins_half = (np.array(hidden_ins) / 2).astype(int)\n","\n","        self.res_blocks_seq = self._get_res_blocks(num_blocks, hidden_ins_half, hiddens_half)\n","        self.res_blocks_epi = self._get_res_blocks(num_blocks, hidden_ins_half, hiddens_half)\n","\n","        self.conv_end = nn.Conv1d(256, output_size, 1)\n","\n","    def forward(self, x):\n","        seq = x[:, :5, :]\n","        epi = x[:, 5:, :]\n","\n","        seq = self.res_blocks_seq(self.conv_start_seq(seq))\n","        epi = self.res_blocks_epi(self.conv_start_epi(epi))\n","\n","        x = torch.cat([seq, epi], dim=1)\n","        return self.conv_end(x)\n","\n","    def _get_res_blocks(self, n, his, hs):\n","        blocks = []\n","        for hi, h in zip(his, hs):\n","            blocks.append(ConvBlock(self.filter_size, hidden_in=hi, hidden=h))\n","        return nn.Sequential(*blocks)\n","\n","\n","class PositionalEncoding(nn.Module):\n","    def __init__(self, hidden, dropout=0.1, max_len=2048):\n","        super().__init__()\n","        self.dropout = nn.Dropout(p=dropout)\n","        position = torch.arange(max_len).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, hidden, 2) * (-np.log(10000.0) / hidden))\n","        pe = torch.zeros(max_len, 1, hidden)\n","        pe[:, 0, 0::2] = torch.sin(position * div_term)\n","        pe[:, 0, 1::2] = torch.cos(position * div_term)\n","        self.register_buffer('pe', pe)\n","\n","    def forward(self, x):\n","        seq_len = x.size(1)\n","        if seq_len > self.pe.size(0):\n","            position = torch.arange(seq_len, device=x.device).unsqueeze(1)\n","            div_term = torch.exp(torch.arange(0, x.size(2), 2, device=x.device) * (-np.log(10000.0) / x.size(2)))\n","            pe = torch.zeros(seq_len, 1, x.size(2), device=x.device)\n","            pe[:, 0, 0::2] = torch.sin(position * div_term)\n","            pe[:, 0, 1::2] = torch.cos(position * div_term)\n","            x = x + pe.squeeze(1)\n","        else:\n","            x = x + self.pe[:seq_len].squeeze(1)\n","        return self.dropout(x)\n","\n","\n","class TransformerModule(nn.Module):\n","    def __init__(self, hidden=256, layers=8):\n","        super().__init__()\n","        self.pos_encoder = PositionalEncoding(hidden, dropout=0.1, max_len=2048)\n","        encoder_layer = nn.TransformerEncoderLayer(\n","            d_model=hidden,\n","            nhead=8,\n","            dropout=0.1,\n","            dim_feedforward=512,\n","            batch_first=True,\n","            norm_first=True\n","        )\n","        self.transformer = nn.TransformerEncoder(encoder_layer, layers)\n","\n","    def forward(self, x):\n","        x = self.pos_encoder(x)\n","        return self.transformer(x)\n","\n","\n","class ResBlockDilated(nn.Module):\n","    def __init__(self, size, hidden=64, dil=2):\n","        super().__init__()\n","        pad_len = dil\n","        self.res = nn.Sequential(\n","            nn.Conv2d(hidden, hidden, size, padding=pad_len, dilation=dil),\n","            nn.BatchNorm2d(hidden),\n","            nn.ReLU(),\n","            nn.Conv2d(hidden, hidden, size, padding=pad_len, dilation=dil),\n","            nn.BatchNorm2d(hidden),\n","        )\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, x):\n","        identity = x\n","        res_out = self.res(x)\n","        return self.relu(res_out + identity)\n","\n","\n","class Decoder(nn.Module):\n","    def __init__(self, in_channel, hidden=256, filter_size=3, num_blocks=5, output_size=256):\n","        super().__init__()\n","        self.output_size = output_size\n","\n","        self.conv_start = nn.Sequential(\n","            nn.Conv2d(in_channel, hidden, 3, 1, 1),\n","            nn.BatchNorm2d(hidden),\n","            nn.ReLU(),\n","        )\n","\n","        self.res_blocks = self._get_res_blocks(num_blocks, hidden)\n","        self.conv_end = nn.Conv2d(hidden, 1, 1)\n","\n","    def forward(self, x):\n","        x = self.conv_start(x)\n","        x = self.res_blocks(x)\n","        x = self.conv_end(x).squeeze(1)\n","\n","        if x.size(-1) != self.output_size or x.size(-2) != self.output_size:\n","            x = nn.functional.interpolate(\n","                x.unsqueeze(1),\n","                size=(self.output_size, self.output_size),\n","                mode='bilinear',\n","                align_corners=False\n","            ).squeeze(1)\n","\n","        return x\n","\n","    def _get_res_blocks(self, n, hidden):\n","        blocks = []\n","        for i in range(n):\n","            dilation = 2 ** (i + 1)\n","            blocks.append(ResBlockDilated(3, hidden=hidden, dil=dilation))\n","        return nn.Sequential(*blocks)\n","\n","\n","class ConvTransHiCModel(nn.Module):\n","    \"\"\"Full C.Origami-style model: Encoder + Transformer + Decoder\"\"\"\n","    def __init__(self, num_genomic_features=4, mid_hidden=256, output_size=256):\n","        super().__init__()\n","        self.encoder = EncoderSplit(num_genomic_features, output_size=mid_hidden, num_blocks=12)\n","        self.transformer = TransformerModule(hidden=mid_hidden, layers=8)\n","\n","        self.pre_decoder_conv = nn.Sequential(\n","            nn.Conv1d(mid_hidden, mid_hidden, kernel_size=3, stride=2, padding=1),\n","            nn.BatchNorm1d(mid_hidden),\n","            nn.ReLU(),\n","            nn.Conv1d(mid_hidden, mid_hidden, kernel_size=3, stride=2, padding=1),\n","            nn.BatchNorm1d(mid_hidden),\n","            nn.ReLU(),\n","        )\n","\n","        self.decoder = Decoder(mid_hidden * 2, hidden=256, output_size=output_size)\n","\n","    def forward(self, x):\n","        x = x.float()\n","        x = self.encoder(x)\n","        x = x.transpose(1, 2)\n","        x = self.transformer(x)\n","        x = x.transpose(1, 2)\n","        x = self.pre_decoder_conv(x)\n","        x = self._diagonalize(x)\n","        return self.decoder(x)\n","\n","    def _diagonalize(self, x):\n","        x_i = x.unsqueeze(3).repeat(1, 1, 1, x.size(2))\n","        x_j = x.unsqueeze(2).repeat(1, 1, x.size(2), 1)\n","        return torch.cat([x_i, x_j], dim=1)\n","\n","\n","# ================================================================\n","# Dataset\n","# ================================================================\n","\n","import gzip\n","import pyBigWig as pbw\n","from skimage.transform import resize\n","\n","\n","class SequenceFeature:\n","    def __init__(self, path):\n","        print(f'Reading sequence: {path}')\n","        with gzip.open(path, 'r') as f:\n","            seq = f.read().decode(\"utf-8\")\n","            seq = seq[seq.find('\\n')+1:].replace('\\n', '').lower()\n","        self.seq = seq\n","\n","    def get(self, start, end):\n","        seq = self.seq[start:end]\n","        en_dict = {'a': 0, 't': 1, 'c': 2, 'g': 3, 'n': 4}\n","        idx = np.array([en_dict.get(ch, 4) for ch in seq], dtype=int)\n","        onehot = np.zeros((len(seq), 5), dtype=np.float32)\n","        if len(seq) > 0:\n","            onehot[np.arange(len(seq)), idx] = 1.0\n","        return onehot\n","\n","    def __len__(self):\n","        return len(self.seq)\n","\n","\n","class HiCFeature:\n","    def __init__(self, path):\n","        print(f'Reading Hi-C: {path}')\n","        self.hic = dict(np.load(path))\n","\n","    def get(self, start, window=2097152, res=10000):\n","        start_bin = int(start / res)\n","        end_bin = start_bin + int(window / res)\n","        return self._diag_to_mat(self.hic, start_bin, end_bin)\n","\n","    def _diag_to_mat(self, ori_load, start, end):\n","        square_len = end - start\n","        diag_load = {}\n","        for d in range(square_len):\n","            diag_load[str(d)] = ori_load[str(d)][start:start + (square_len - d)]\n","            diag_load[str(-d)] = ori_load[str(-d)][start:start + (square_len - d)]\n","\n","        rows = []\n","        for d in range(square_len):\n","            row = []\n","            for line in range(-d, -d + square_len):\n","                if line < 0:\n","                    row.append(diag_load[str(line)][line + d])\n","                else:\n","                    row.append(diag_load[str(line)][d])\n","            rows.append(row)\n","        return np.array(rows, dtype=np.float32)\n","\n","    def __len__(self):\n","        return len(self.hic['0'])\n","\n","\n","class GenomicFeature:\n","    def __init__(self, path, norm):\n","        self.path = path\n","        self.norm = norm\n","        print(f'Feature: {path}, Norm: {norm}')\n","\n","    def get(self, chr_name, start, end):\n","        with pbw.open(self.path) as bw:\n","            signals = np.array(bw.values(chr_name, int(start), int(end)))\n","            signals = np.nan_to_num(signals, 0.0)\n","            if self.norm == 'log':\n","                signals = np.log(signals + 1)\n","            return signals.astype(np.float32)\n","\n","    def length(self, chr_name):\n","        with pbw.open(self.path) as bw:\n","            return bw.chroms(chr_name)\n","\n","\n","class ChromosomeDataset(torch.utils.data.Dataset):\n","    def __init__(self, celltype_root, chr_name, omit_regions, feature_list, use_aug=True):\n","        self.res = 10000\n","        self.sample_bins = 500\n","        self.stride = 50\n","        self.image_scale = 256\n","        self.chr_name = chr_name\n","        self.use_aug = use_aug\n","\n","        print(f'Loading {chr_name}...')\n","        self.seq = SequenceFeature(f'{celltype_root}/../dna_sequence/{chr_name}.fa.gz')\n","        self.genomic_features = feature_list\n","        self.mat = HiCFeature(f'{celltype_root}/hic_matrix/{chr_name}.npz')\n","        self.omit_regions = omit_regions\n","\n","        self._check_length()\n","        all_intervals = self._get_intervals()\n","        self.intervals = self._filter(all_intervals, omit_regions)\n","\n","    def __len__(self):\n","        return len(self.intervals)\n","\n","    def __getitem__(self, idx):\n","        start, end = self.intervals[idx]\n","        target_size = int(self.sample_bins * self.res)\n","\n","        if self.use_aug:\n","            offset = np.random.randint(0, end - start - target_size + 1)\n","        else:\n","            offset = 0\n","\n","        start, end = start + offset, start + offset + target_size\n","\n","        seq = self.seq.get(start, end)\n","        features = [f.get(self.chr_name, start, end) for f in self.genomic_features]\n","        mat = self.mat.get(start)\n","        mat = resize(mat, (self.image_scale, self.image_scale), anti_aliasing=True)\n","        mat = np.log(mat + 1).astype(np.float32)\n","\n","        if self.use_aug:\n","            seq = self._gaussian_noise(seq, 0.1)\n","            features = [self._gaussian_noise(f, 0.1) for f in features]\n","            seq, features, mat = self._reverse_complement(seq, features, mat)\n","\n","        return seq, features, mat\n","\n","    def _gaussian_noise(self, x, std=0.1):\n","        return x + np.random.randn(*x.shape).astype(np.float32) * std\n","\n","    def _reverse_complement(self, seq, features, mat, chance=0.5):\n","        if np.random.rand() < chance:\n","            seq = np.flip(seq, 0).copy()\n","            seq = np.concatenate([seq[:, 1:2], seq[:, 0:1], seq[:, 3:4], seq[:, 2:3], seq[:, 4:5]], axis=1)\n","            features = [np.flip(f, 0).copy() for f in features]\n","            mat = np.flip(mat, [0, 1]).copy()\n","        return seq, features, mat\n","\n","    def _get_intervals(self):\n","        chr_bins = len(self.seq) / self.res\n","        n = max(0, int((chr_bins - self.sample_bins) / self.stride))\n","        starts = np.arange(n).reshape(-1, 1) * self.stride\n","        bins = np.concatenate([starts, starts + self.sample_bins], axis=1) if n > 0 else np.zeros((0, 2))\n","        return (bins * self.res).astype(int)\n","\n","    def _filter(self, intervals, omit_regions):\n","        if omit_regions is None or len(omit_regions) == 0:\n","            return intervals.tolist()\n","\n","        valid = []\n","        for s, e in intervals:\n","            if np.sum((s <= omit_regions[:, 1]) & (omit_regions[:, 0] <= e)) == 0:\n","                valid.append([int(s), int(e)])\n","        return valid\n","\n","    def _check_length(self):\n","        if self.genomic_features:\n","            assert len(self.seq) == self.genomic_features[0].length(self.chr_name)\n","            assert abs(len(self.seq) / self.res - len(self.mat)) < 2\n","\n","\n","# ================================================================\n","# Evaluation Metrics (SIMPLIFIED FOR SPEED)\n","# ================================================================\n","\n","from scipy.stats import pearsonr, spearmanr\n","from scipy.ndimage import uniform_filter1d\n","\n","\n","def upper_triangle_flatten(mat):\n","    iu = np.triu_indices_from(mat, k=1)\n","    return mat[iu]\n","\n","\n","def compute_global_corr(pred, true):\n","    p, t = upper_triangle_flatten(pred), upper_triangle_flatten(true)\n","    mask = np.isfinite(p) & np.isfinite(t)\n","    if mask.sum() < 3:\n","        return np.nan, np.nan\n","    return pearsonr(p[mask], t[mask])[0], spearmanr(p[mask], t[mask])[0]\n","\n","\n","# ================================================================\n","# Custom Collate Function\n","# ================================================================\n","\n","def collate_fn(batch):\n","    seqs, features_list, mats = zip(*batch)\n","    seqs = torch.from_numpy(np.stack(seqs))\n","\n","    num_features = len(features_list[0])\n","    features = [np.stack([f[i] for f in features_list]) for i in range(num_features)]\n","    features = [torch.from_numpy(f) for f in features]\n","\n","    mats = torch.from_numpy(np.stack(mats))\n","    return seqs, features, mats\n","\n","\n","# ================================================================\n","# PyTorch Lightning Module (SIMPLIFIED FOR SPEED)\n","# ================================================================\n","\n","class HiCTrainingModule(pl.LightningModule):\n","    def __init__(self, args):\n","        super().__init__()\n","        self.save_hyperparameters(ignore=['args'])\n","        self.args = args\n","        # UPDATED: num_genomic_features=4 (CTCF, ATAC, DNAmeth minus, DNAmeth plus)\n","        self.model = ConvTransHiCModel(num_genomic_features=4, mid_hidden=256, output_size=256)\n","\n","    def forward(self, x):\n","        return self.model(x)\n","\n","    def _proc_batch(self, batch):\n","        seq, features, mat = batch\n","        features = torch.stack(features, dim=2)\n","        inputs = torch.cat([seq, features], dim=2)\n","        inputs = inputs.transpose(1, 2)\n","        return inputs, mat\n","\n","    def training_step(self, batch, batch_idx):\n","        inputs, mat = self._proc_batch(batch)\n","        outputs = self(inputs)\n","        loss = nn.functional.mse_loss(outputs, mat)\n","        self.log('train_loss', loss, prog_bar=True)\n","        return loss\n","\n","    def validation_step(self, batch, batch_idx):\n","        inputs, mat = self._proc_batch(batch)\n","        outputs = self(inputs)\n","        loss = nn.functional.mse_loss(outputs, mat)\n","\n","        # SPEED: Only compute correlation on subset of validation\n","        if batch_idx == 0:  # Only first batch\n","            pred = outputs[0].detach().cpu().numpy()\n","            true = mat[0].detach().cpu().numpy()\n","            gp, gs = compute_global_corr(pred, true)\n","            self.log('val_pearson', gp, prog_bar=True)\n","\n","        self.log('val_loss', loss, prog_bar=True)\n","        return loss\n","\n","    def test_step(self, batch, batch_idx):\n","        inputs, mat = self._proc_batch(batch)\n","        outputs = self(inputs)\n","        loss = nn.functional.mse_loss(outputs, mat)\n","        return loss\n","\n","    def configure_optimizers(self):\n","        optimizer = torch.optim.AdamW(self.parameters(), lr=3e-4, weight_decay=1e-5)\n","        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n","            optimizer,\n","            max_lr=3e-4,\n","            total_steps=self.trainer.estimated_stepping_batches,\n","            pct_start=0.1,\n","            anneal_strategy='cos'\n","        )\n","        return {'optimizer': optimizer, 'lr_scheduler': {'scheduler': scheduler, 'interval': 'step'}}\n","\n","\n","# ================================================================\n","# Main Training Script\n","# ================================================================\n","\n","def load_centrotelo(bed_path):\n","    if not os.path.exists(bed_path):\n","        return {}\n","\n","    import pandas as pd\n","    df = pd.read_csv(bed_path, sep='\\t', names=['chr', 'start', 'end'])\n","    result = {}\n","    for chr_name, group in df.groupby('chr'):\n","        result[chr_name] = group[['start', 'end']].to_numpy(dtype=int)\n","    return result\n","\n","\n","def get_available_chromosomes(celltype_root):\n","    hic_dir = f'{celltype_root}/hic_matrix'\n","    available = []\n","    if os.path.exists(hic_dir):\n","        for f in os.listdir(hic_dir):\n","            if f.endswith('.npz'):\n","                chr_name = f.replace('.npz', '')\n","                available.append(chr_name)\n","    return sorted(available)\n","\n","\n","def get_dataset(args, mode):\n","    celltype_root = f'{args.data_root}/{args.assembly}/{args.celltype}'\n","\n","    # UPDATED: Added DNA methylation features (minus and plus strand)\n","    features = [\n","        GenomicFeature(f'{celltype_root}/genomic_features/ctcf_log2fc.bw', norm=None),\n","        GenomicFeature(f'{celltype_root}/genomic_features/atac.bw', norm='log'),\n","        GenomicFeature(f'{celltype_root}/genomic_features/dnameth-minusstrand.bigWig', norm=None),\n","        GenomicFeature(f'{celltype_root}/genomic_features/dnameth-plusstrand.bigWig', norm=None),\n","    ]\n","\n","    centrotelo = load_centrotelo(f'{celltype_root}/../centrotelo.bed')\n","    available_chrs = get_available_chromosomes(celltype_root)\n","\n","    print(f\"\\nAvailable chromosomes: {available_chrs}\")\n","\n","    if mode == 'train':\n","        desired = [f'chr{i}' for i in range(1, 23) if i not in [10, 15]]\n","    elif mode == 'val':\n","        desired = ['chr10']\n","    elif mode == 'test':\n","        desired = ['chr15']\n","    else:\n","        raise ValueError(f'Unknown mode: {mode}')\n","\n","    chr_names = [c for c in desired if c in available_chrs]\n","\n","    if len(chr_names) == 0:\n","        raise ValueError(f\"No valid chromosomes found for mode '{mode}'\")\n","\n","    print(f\"Using chromosomes for {mode}: {chr_names}\\n\")\n","\n","    datasets = []\n","    for chr_name in chr_names:\n","        omit = centrotelo.get(chr_name, np.zeros((0, 2), dtype=int))\n","        use_aug = (mode == 'train')\n","\n","        try:\n","            ds = ChromosomeDataset(celltype_root, chr_name, omit, features, use_aug=use_aug)\n","            datasets.append(ds)\n","            print(f\"✓ Loaded {chr_name}: {len(ds)} samples\")\n","        except FileNotFoundError as e:\n","            print(f\"⚠️ Skipping {chr_name}: File not found\")\n","            continue\n","        except Exception as e:\n","            print(f\"⚠️ Skipping {chr_name}: {str(e)}\")\n","            continue\n","\n","    if len(datasets) == 0:\n","        raise ValueError(f\"No datasets loaded for mode '{mode}'\")\n","\n","    print(f\"Total {mode} samples: {sum(len(d) for d in datasets)}\\n\")\n","    return torch.utils.data.ConcatDataset(datasets)\n","\n","\n","def main():\n","    # Clear memory first\n","    torch.cuda.empty_cache()\n","    gc.collect()\n","\n","    # Check GPU availability\n","    print(\"=\"*70)\n","    print(\"SYSTEM CHECK\")\n","    print(\"=\"*70)\n","    print(f\"PyTorch version: {torch.__version__}\")\n","    print(f\"CUDA available: {torch.cuda.is_available()}\")\n","    if torch.cuda.is_available():\n","        print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n","        print(f\"CUDA version: {torch.version.cuda}\")\n","    else:\n","        print(\"⚠️ WARNING: No GPU detected! Training will be VERY slow on CPU.\")\n","        print(\"   To enable GPU in Colab: Runtime > Change runtime type > Hardware accelerator > GPU\")\n","    print(\"=\"*70)\n","\n","    # Enable optimizations for A100\n","    torch.backends.cudnn.benchmark = True\n","    torch.set_float32_matmul_precision('high')\n","\n","    class Args:\n","        data_root = \"/content/drive/MyDrive/ML4GEN DATA/data - IMR90\"\n","        assembly = \"hg38\"\n","        celltype = \"imr90\"\n","        batch_size = 12\n","        num_workers = 2  # Reduced from 4 to 2 as per warning\n","        max_epochs = 10\n","        patience = 5\n","        num_gpus = 1 if torch.cuda.is_available() else 0  # Auto-detect GPU\n","        save_path = \"/content/drive/MyDrive/ML4GEN DATA/data - IMR90/checkpoints_dna_meth\"\n","        save_top_k = 3\n","\n","    args = Args()\n","\n","    pl.seed_everything(2077, workers=True)\n","\n","    os.makedirs(f'{args.save_path}/models', exist_ok=True)\n","    os.makedirs(f'{args.save_path}/logs', exist_ok=True)\n","\n","    print(\"=\"*70)\n","    print(\"LOADING DATASETS\")\n","    print(\"=\"*70)\n","\n","    train_ds = get_dataset(args, 'train')\n","    val_ds = get_dataset(args, 'val')\n","    test_ds = get_dataset(args, 'test')\n","\n","    train_loader = DataLoader(\n","        train_ds,\n","        batch_size=args.batch_size,\n","        shuffle=True,\n","        num_workers=args.num_workers,\n","        pin_memory=True,\n","        persistent_workers=True,\n","        collate_fn=collate_fn,\n","        prefetch_factor=2,\n","    )\n","\n","    val_loader = DataLoader(\n","        val_ds,\n","        batch_size=args.batch_size,\n","        shuffle=False,\n","        num_workers=args.num_workers,\n","        pin_memory=True,\n","        persistent_workers=True,\n","        collate_fn=collate_fn,\n","        prefetch_factor=2,\n","    )\n","\n","    test_loader = DataLoader(\n","        test_ds,\n","        batch_size=args.batch_size,\n","        shuffle=False,\n","        num_workers=2,\n","        pin_memory=True,\n","        collate_fn=collate_fn\n","    )\n","\n","    model = HiCTrainingModule(args)\n","\n","    checkpoint_cb = callbacks.ModelCheckpoint(\n","        dirpath=f'{args.save_path}/models',\n","        filename='hic-{epoch:02d}-{val_loss:.4f}',\n","        monitor='val_loss',\n","        save_top_k=args.save_top_k,\n","        mode='min',\n","        save_last=True,\n","    )\n","\n","    early_stop_cb = callbacks.EarlyStopping(\n","        monitor='val_loss',\n","        patience=args.patience,\n","        mode='min'\n","    )\n","\n","    lr_monitor = callbacks.LearningRateMonitor(logging_interval='step')\n","    csv_logger = pl.loggers.CSVLogger(save_dir=f'{args.save_path}/logs')\n","\n","    trainer = pl.Trainer(\n","        accelerator='gpu' if torch.cuda.is_available() else 'cpu',\n","        devices=args.num_gpus if args.num_gpus > 0 else 'auto',\n","        max_epochs=args.max_epochs,\n","        callbacks=[checkpoint_cb, early_stop_cb, lr_monitor],\n","        logger=csv_logger,\n","        gradient_clip_val=1.0,\n","        precision='16-mixed' if torch.cuda.is_available() else '32',\n","        enable_progress_bar=True,\n","        enable_model_summary=True,\n","        log_every_n_steps=10,\n","        val_check_interval=0.25,\n","    )\n","\n","    print(\"\\n\" + \"=\"*70)\n","    print(\"STARTING TRAINING\")\n","    print(f\"Batch size: {args.batch_size}\")\n","    print(f\"Max epochs: {args.max_epochs}\")\n","    print(f\"Training samples: {len(train_ds)}\")\n","    print(\"=\"*70)\n","\n","    trainer.fit(model, train_loader, val_loader)\n","\n","    print(\"\\n\" + \"=\"*70)\n","    print(\"TESTING BEST MODEL\")\n","    print(\"=\"*70)\n","\n","    trainer.test(model, test_loader, ckpt_path='best')\n","\n","    print(f\"\\n✅ Training complete! Best model saved to {args.save_path}/models\")\n","\n","\n","if __name__ == '__main__':\n","    main()"],"metadata":{"id":"ivmDsvM9Xim_","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["8337407c76fb4519bb60a1970244f91b","181dff1a71e94c2f8d959da581fc4163"]},"outputId":"7cb67e74-1464-4b6d-d8de-1f2d3ef11e09"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n","  _C._set_float32_matmul_precision(precision)\n","INFO:lightning_fabric.utilities.seed:Seed set to 2077\n"]},{"output_type":"stream","name":"stdout","text":["======================================================================\n","SYSTEM CHECK\n","======================================================================\n","PyTorch version: 2.9.0+cu126\n","CUDA available: True\n","CUDA device: NVIDIA A100-SXM4-40GB\n","CUDA version: 12.6\n","======================================================================\n","======================================================================\n","LOADING DATASETS\n","======================================================================\n","Feature: /content/drive/MyDrive/ML4GEN DATA/data - IMR90/hg38/imr90/genomic_features/ctcf_log2fc.bw, Norm: None\n","Feature: /content/drive/MyDrive/ML4GEN DATA/data - IMR90/hg38/imr90/genomic_features/atac.bw, Norm: log\n","Feature: /content/drive/MyDrive/ML4GEN DATA/data - IMR90/hg38/imr90/genomic_features/dnameth-minusstrand.bigWig, Norm: None\n","Feature: /content/drive/MyDrive/ML4GEN DATA/data - IMR90/hg38/imr90/genomic_features/dnameth-plusstrand.bigWig, Norm: None\n","\n","Available chromosomes: ['chr1', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr2', 'chr20', 'chr21', 'chr22', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chrX']\n","Using chromosomes for train: ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr11', 'chr12', 'chr13', 'chr14', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22']\n","\n","Loading chr1...\n","Reading sequence: /content/drive/MyDrive/ML4GEN DATA/data - IMR90/hg38/imr90/../dna_sequence/chr1.fa.gz\n","Reading Hi-C: /content/drive/MyDrive/ML4GEN DATA/data - IMR90/hg38/imr90/hic_matrix/chr1.npz\n","✓ Loaded chr1: 471 samples\n","Loading chr2...\n","Reading sequence: /content/drive/MyDrive/ML4GEN DATA/data - IMR90/hg38/imr90/../dna_sequence/chr2.fa.gz\n","Reading Hi-C: /content/drive/MyDrive/ML4GEN DATA/data - IMR90/hg38/imr90/hic_matrix/chr2.npz\n","✓ Loaded chr2: 459 samples\n","Loading chr3...\n","Reading sequence: /content/drive/MyDrive/ML4GEN DATA/data - IMR90/hg38/imr90/../dna_sequence/chr3.fa.gz\n","Reading Hi-C: /content/drive/MyDrive/ML4GEN DATA/data - IMR90/hg38/imr90/hic_matrix/chr3.npz\n","✓ Loaded chr3: 369 samples\n","Loading chr4...\n","Reading sequence: /content/drive/MyDrive/ML4GEN DATA/data - IMR90/hg38/imr90/../dna_sequence/chr4.fa.gz\n","Reading Hi-C: /content/drive/MyDrive/ML4GEN DATA/data - IMR90/hg38/imr90/hic_matrix/chr4.npz\n","✓ Loaded chr4: 355 samples\n","Loading chr5...\n","Reading sequence: /content/drive/MyDrive/ML4GEN DATA/data - IMR90/hg38/imr90/../dna_sequence/chr5.fa.gz\n","Reading Hi-C: /content/drive/MyDrive/ML4GEN DATA/data - IMR90/hg38/imr90/hic_matrix/chr5.npz\n","✓ Loaded chr5: 334 samples\n","Loading chr6...\n","Reading sequence: /content/drive/MyDrive/ML4GEN DATA/data - IMR90/hg38/imr90/../dna_sequence/chr6.fa.gz\n","Reading Hi-C: /content/drive/MyDrive/ML4GEN DATA/data - IMR90/hg38/imr90/hic_matrix/chr6.npz\n","✓ Loaded chr6: 318 samples\n","Loading chr7...\n","Reading sequence: /content/drive/MyDrive/ML4GEN DATA/data - IMR90/hg38/imr90/../dna_sequence/chr7.fa.gz\n","Reading Hi-C: /content/drive/MyDrive/ML4GEN DATA/data - IMR90/hg38/imr90/hic_matrix/chr7.npz\n","✓ Loaded chr7: 290 samples\n","Loading chr8...\n","Reading sequence: /content/drive/MyDrive/ML4GEN DATA/data - IMR90/hg38/imr90/../dna_sequence/chr8.fa.gz\n","Reading Hi-C: /content/drive/MyDrive/ML4GEN DATA/data - IMR90/hg38/imr90/hic_matrix/chr8.npz\n","✓ Loaded chr8: 266 samples\n","Loading chr9...\n","Reading sequence: /content/drive/MyDrive/ML4GEN DATA/data - IMR90/hg38/imr90/../dna_sequence/chr9.fa.gz\n","Reading Hi-C: /content/drive/MyDrive/ML4GEN DATA/data - IMR90/hg38/imr90/hic_matrix/chr9.npz\n","✓ Loaded chr9: 250 samples\n","Loading chr11...\n","Reading sequence: /content/drive/MyDrive/ML4GEN DATA/data - IMR90/hg38/imr90/../dna_sequence/chr11.fa.gz\n","Reading Hi-C: /content/drive/MyDrive/ML4GEN DATA/data - IMR90/hg38/imr90/hic_matrix/chr11.npz\n","✓ Loaded chr11: 243 samples\n","Loading chr12...\n","Reading sequence: /content/drive/MyDrive/ML4GEN DATA/data - IMR90/hg38/imr90/../dna_sequence/chr12.fa.gz\n","Reading Hi-C: /content/drive/MyDrive/ML4GEN DATA/data - IMR90/hg38/imr90/hic_matrix/chr12.npz\n","✓ Loaded chr12: 240 samples\n","Loading chr13...\n","Reading sequence: /content/drive/MyDrive/ML4GEN DATA/data - IMR90/hg38/imr90/../dna_sequence/chr13.fa.gz\n","Reading Hi-C: /content/drive/MyDrive/ML4GEN DATA/data - IMR90/hg38/imr90/hic_matrix/chr13.npz\n","✓ Loaded chr13: 202 samples\n","Loading chr14...\n","Reading sequence: /content/drive/MyDrive/ML4GEN DATA/data - IMR90/hg38/imr90/../dna_sequence/chr14.fa.gz\n","Reading Hi-C: /content/drive/MyDrive/ML4GEN DATA/data - IMR90/hg38/imr90/hic_matrix/chr14.npz\n","✓ Loaded chr14: 188 samples\n","Loading chr16...\n","Reading sequence: /content/drive/MyDrive/ML4GEN DATA/data - IMR90/hg38/imr90/../dna_sequence/chr16.fa.gz\n","Reading Hi-C: /content/drive/MyDrive/ML4GEN DATA/data - IMR90/hg38/imr90/hic_matrix/chr16.npz\n","✓ Loaded chr16: 155 samples\n","Loading chr17...\n","Reading sequence: /content/drive/MyDrive/ML4GEN DATA/data - IMR90/hg38/imr90/../dna_sequence/chr17.fa.gz\n","Reading Hi-C: /content/drive/MyDrive/ML4GEN DATA/data - IMR90/hg38/imr90/hic_matrix/chr17.npz\n","✓ Loaded chr17: 137 samples\n","Loading chr18...\n","Reading sequence: /content/drive/MyDrive/ML4GEN DATA/data - IMR90/hg38/imr90/../dna_sequence/chr18.fa.gz\n","Reading Hi-C: /content/drive/MyDrive/ML4GEN DATA/data - IMR90/hg38/imr90/hic_matrix/chr18.npz\n","✓ Loaded chr18: 128 samples\n","Loading chr19...\n","Reading sequence: /content/drive/MyDrive/ML4GEN DATA/data - IMR90/hg38/imr90/../dna_sequence/chr19.fa.gz\n","Reading Hi-C: /content/drive/MyDrive/ML4GEN DATA/data - IMR90/hg38/imr90/hic_matrix/chr19.npz\n","✓ Loaded chr19: 90 samples\n","Loading chr20...\n","Reading sequence: /content/drive/MyDrive/ML4GEN DATA/data - IMR90/hg38/imr90/../dna_sequence/chr20.fa.gz\n","Reading Hi-C: /content/drive/MyDrive/ML4GEN DATA/data - IMR90/hg38/imr90/hic_matrix/chr20.npz\n","✓ Loaded chr20: 99 samples\n","Loading chr21...\n","Reading sequence: /content/drive/MyDrive/ML4GEN DATA/data - IMR90/hg38/imr90/../dna_sequence/chr21.fa.gz\n","Reading Hi-C: /content/drive/MyDrive/ML4GEN DATA/data - IMR90/hg38/imr90/hic_matrix/chr21.npz\n","✓ Loaded chr21: 68 samples\n","Loading chr22...\n","Reading sequence: /content/drive/MyDrive/ML4GEN DATA/data - IMR90/hg38/imr90/../dna_sequence/chr22.fa.gz\n","Reading Hi-C: /content/drive/MyDrive/ML4GEN DATA/data - IMR90/hg38/imr90/hic_matrix/chr22.npz\n","✓ Loaded chr22: 75 samples\n","Total train samples: 4737\n","\n","Feature: /content/drive/MyDrive/ML4GEN DATA/data - IMR90/hg38/imr90/genomic_features/ctcf_log2fc.bw, Norm: None\n","Feature: /content/drive/MyDrive/ML4GEN DATA/data - IMR90/hg38/imr90/genomic_features/atac.bw, Norm: log\n","Feature: /content/drive/MyDrive/ML4GEN DATA/data - IMR90/hg38/imr90/genomic_features/dnameth-minusstrand.bigWig, Norm: None\n","Feature: /content/drive/MyDrive/ML4GEN DATA/data - IMR90/hg38/imr90/genomic_features/dnameth-plusstrand.bigWig, Norm: None\n","\n","Available chromosomes: ['chr1', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr2', 'chr20', 'chr21', 'chr22', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chrX']\n","Using chromosomes for val: ['chr10']\n","\n","Loading chr10...\n","Reading sequence: /content/drive/MyDrive/ML4GEN DATA/data - IMR90/hg38/imr90/../dna_sequence/chr10.fa.gz\n","Reading Hi-C: /content/drive/MyDrive/ML4GEN DATA/data - IMR90/hg38/imr90/hic_matrix/chr10.npz\n","✓ Loaded chr10: 242 samples\n","Total val samples: 242\n","\n","Feature: /content/drive/MyDrive/ML4GEN DATA/data - IMR90/hg38/imr90/genomic_features/ctcf_log2fc.bw, Norm: None\n","Feature: /content/drive/MyDrive/ML4GEN DATA/data - IMR90/hg38/imr90/genomic_features/atac.bw, Norm: log\n","Feature: /content/drive/MyDrive/ML4GEN DATA/data - IMR90/hg38/imr90/genomic_features/dnameth-minusstrand.bigWig, Norm: None\n","Feature: /content/drive/MyDrive/ML4GEN DATA/data - IMR90/hg38/imr90/genomic_features/dnameth-plusstrand.bigWig, Norm: None\n","\n","Available chromosomes: ['chr1', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr2', 'chr20', 'chr21', 'chr22', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chrX']\n","Using chromosomes for test: ['chr15']\n","\n","Loading chr15...\n","Reading sequence: /content/drive/MyDrive/ML4GEN DATA/data - IMR90/hg38/imr90/../dna_sequence/chr15.fa.gz\n","Reading Hi-C: /content/drive/MyDrive/ML4GEN DATA/data - IMR90/hg38/imr90/hic_matrix/chr15.npz\n","✓ Loaded chr15: 177 samples\n","Total test samples: 177\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n","  warnings.warn(\n","INFO:pytorch_lightning.utilities.rank_zero:Using 16bit Automatic Mixed Precision (AMP)\n","INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n","INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n"]},{"output_type":"stream","name":"stdout","text":["\n","======================================================================\n","STARTING TRAINING\n","Batch size: 12\n","Max epochs: 10\n","Training samples: 4737\n","======================================================================\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py:881: Checkpoint directory /content/drive/.shortcut-targets-by-id/126vLgWy4wFKfcpk6pUYC6UiOP4G1DHi2/ML4GEN DATA/data - IMR90/checkpoints_dna_meth/models exists and is not empty.\n","INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","INFO:pytorch_lightning.utilities.rank_zero:Loading `train_dataloader` to estimate number of stepping batches.\n","/usr/local/lib/python3.12/dist-packages/pytorch_lightning/utilities/model_summary/model_summary.py:242: Precision 16-mixed is not supported by the model summary.  Estimated model size in MB will not be accurate. Using 32 bits instead.\n"]},{"output_type":"display_data","data":{"text/plain":["┏━━━┳━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┳━━━━━━━┓\n","┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType             \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mMode \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mFLOPs\u001b[0m\u001b[1;35m \u001b[0m┃\n","┡━━━╇━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━╇━━━━━━━┩\n","│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ model │ ConvTransHiCModel │ 13.2 M │ train │     0 │\n","└───┴───────┴───────────────────┴────────┴───────┴───────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┳━━━━━━━┓\n","┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name  </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type              </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Mode  </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> FLOPs </span>┃\n","┡━━━╇━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━╇━━━━━━━┩\n","│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ model │ ConvTransHiCModel │ 13.2 M │ train │     0 │\n","└───┴───────┴───────────────────┴────────┴───────┴───────┘\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1mTrainable params\u001b[0m: 13.2 M                                                                                           \n","\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n","\u001b[1mTotal params\u001b[0m: 13.2 M                                                                                               \n","\u001b[1mTotal estimated model params size (MB)\u001b[0m: 52                                                                         \n","\u001b[1mModules in train mode\u001b[0m: 440                                                                                         \n","\u001b[1mModules in eval mode\u001b[0m: 0                                                                                            \n","\u001b[1mTotal FLOPs\u001b[0m: 0                                                                                                     \n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 13.2 M                                                                                           \n","<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n","<span style=\"font-weight: bold\">Total params</span>: 13.2 M                                                                                               \n","<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 52                                                                         \n","<span style=\"font-weight: bold\">Modules in train mode</span>: 440                                                                                         \n","<span style=\"font-weight: bold\">Modules in eval mode</span>: 0                                                                                            \n","<span style=\"font-weight: bold\">Total FLOPs</span>: 0                                                                                                     \n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Output()"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8337407c76fb4519bb60a1970244f91b"}},"metadata":{}}]}]}